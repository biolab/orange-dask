{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "import pandas as pd\n",
    "import dask.dataframe as dd\n",
    "import dask.array as da\n",
    "import numpy as np\n",
    "import zarr\n",
    "import numcodecs\n",
    "import string\n",
    "import pickle\n",
    "\n",
    "from numpy.lib.recfunctions import unstructured_to_structured\n",
    "from Orange.data import Table, Domain\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 1 - HDF5\n",
    "\n",
    "Covert metas into a structured array and store them as such in HDF5 file format.\n",
    "\n",
    "Prior to storing strings on a disk, we need to encode them. \n",
    "[From H5PY docs:](https://docs.h5py.org/en/stable/strings.html#strings)\n",
    "\n",
    "> When writing data to an existing dataset or attribute, data passed as bytes are written without checking the encoding. Data passed as Python str objects are encoded as either ASCII or UTF-8, based on the HDF5 datatype.\n",
    "\n",
    "This means that we have to be aware of the encoding when reading data from the disk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_domain = Table('iris').domain\n",
    "data = np.random.random((50000, 10000))\n",
    "codes = np.random.choice(list(string.printable) + list('ČĆŽĐŠžćčđš'), size=[50000, 10])\n",
    "ecoded_string = np.char.encode([''.join(code) for code in codes], encoding='iso8859_2')\n",
    "metas = np.column_stack((np.random.randint(2, size=50000), np.random.randint(100, size=50000),  np.array(ecoded_string, dtype=object)))\n",
    "\n",
    "structured_metas = np.array([tuple(row) for row in metas], dtype=np.dtype([('meta1', int), ('meta2', float), ('meta3', h5py.string_dtype())]))\n",
    "with h5py.File('example-1.h5', 'w') as f:\n",
    "    f.create_dataset('X', data=data)\n",
    "    f.create_dataset('metas', data=structured_metas)\n",
    "    f.create_dataset('domain', data=np.void(pickle.dumps(example_domain)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "with h5py.File('example-1.h5', 'r') as f:\n",
    "    X = da.from_array(f['X'])\n",
    "    # when creating a Dask array, we have to specify chunk size. Dask can't perform auto-chunking when given object data.\n",
    "    # NotImplementedError: Can not use auto rechunking with object dtype. We are unable to estimate the size in bytes of object data\n",
    "    M = da.from_array(f['metas'], chunks=(-1,))\n",
    "    # We can also read this as a Dask data frame:\n",
    "    M_df = dd.from_array(f['metas'])\n",
    "\n",
    "    # encoding/decoding sanity check\n",
    "    a = [''.join(code) for code in codes]\n",
    "    b = [x.decode('iso8859_2') for x in f['metas']['meta3'][()]]\n",
    "    print(a == b)\n",
    "\n",
    "# print(X, M, M_df, domain)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 2 - Zarr\n",
    "\n",
    "Zarr file format can be a drop-in replacement for HDF5 with a similar API. It supports arrays of type Object by default; you only need to specify how it will be encoded/decoded (see example below).\n",
    "\n",
    "Zarr allows us to read the .metas as object arrays directly into Dask, so in theory, no significant changes will be needed to support this in Orange. For example, if we stick with HDF, we will have to take into account an alternative representation of metas, that is, structured arrays.\n",
    "\n",
    "Things that we should be aware of:\n",
    "\n",
    "- Zarr (by default) reads/writes data into a directory and not a single file. [The storage alternative](https://zarr.readthedocs.io/en/stable/tutorial.html?highlight=chunk#storage-alternatives) that is interesting for us at this point is using a ZIP file (I don't know if there are noticeable performance differences between the two). In the example below, we zip the folder with ```7z a -tzip example-2.zarr.zip example-2.zarr/.``` and open the file as we would usually do.\n",
    "\n",
    "\n",
    "- When using Zarr, we must store the domain as a dataset because internally, Zarr uses JSON to store array attributes, so attribute values must be JSON serializable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_domain = Table('iris').domain\n",
    "data = np.random.random((50000, 10000))\n",
    "codes = np.random.choice(list(string.printable) + list('ČĆŽĐŠžćčđš'), size=[50000, 10])\n",
    "metas = np.column_stack((np.random.randint(2, size=50000), np.random.randint(100, size=50000),  np.array([''.join(code) for code in codes], dtype=object)))\n",
    "\n",
    "with zarr.open('example-2.zarr', 'w') as f:\n",
    "    table = f.create_group('table')\n",
    "    table.create_dataset('X', data=data)\n",
    "    table.create_dataset('metas', data=metas, object_codec=numcodecs.MsgPack())\n",
    "\n",
    "    # Store domain variables in separate datasets in groups. An array of variables are read from the disk.\n",
    "    domain = f.create_group('domain')\n",
    "    domain.create_dataset('attributes', data=example_domain.attributes, object_codec=numcodecs.Pickle())\n",
    "    domain.create_dataset('class_vars', data=example_domain.class_vars, object_codec=numcodecs.Pickle())\n",
    "    domain.create_dataset('metas', data=example_domain.metas, object_codec=numcodecs.Pickle())\n",
    "\n",
    "    # Alternativly we could also use:\n",
    "    domain = f.create_dataset('domain_2', data=pickle.dumps(example_domain))\n",
    "    # or\n",
    "    domain = f.create_dataset('domain_3', data=example_domain, object_codec=numcodecs.Pickle())\n",
    "\n",
    "\n",
    "# or use zarr.zip\n",
    "with zarr.open('example-2.zarr', 'r') as f:\n",
    "    X = da.from_array(f['table']['X'])\n",
    "    M = da.from_array(f['table']['metas'], chunks=(-1, 1))\n",
    "    # We can also read this as a Dask data frame:\n",
    "    M_df = dd.from_array(f['table']['metas'])\n",
    "\n",
    "\n",
    "    domain = Domain(f['domain']['attributes'][()],  # Domain\n",
    "                    f['domain']['class_vars'][()], \n",
    "                    f['domain']['metas'][()])\n",
    "\n",
    "    domain_2 = pickle.loads(f['domain_2'][()])  # Domain\n",
    "    domain_3 = f['domain_3'][()] # This is a an array of domain varaibles."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.9 ('orange')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "34ee6e43fd02f1df4322c48b94db4849a68f55f527a612f8b196227417e919f6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
